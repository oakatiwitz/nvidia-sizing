{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6296d2",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30afe4d4",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\">**Notebook 0:** First Contact With NIMs</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a43b9a",
   "metadata": {},
   "source": [
    "**Welcome to the introductory notebook on NVIDIA Inference Microservices (NIMs).** In this notebook, we will explore how to interact with a NIM endpoint, specifically focusing on the Llama 3 8B model. By the end of this notebook, you will have a foundational understanding of NIMs and how to perform basic tasks such as checking the status of a NIM endpoint, querying available models, and generating text.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Interact with a NIM endpoint to check its status and the available models.\n",
    "- Call a NIM endpoint of Llama 3 8B to generate text using curl and Python.\n",
    "- Understand the difference between end-to-end latency and time to first token.\n",
    "\n",
    "**Before starting this notebook, please make sure to watch its corresponding video.**\n",
    "\n",
    "## Table of Contents\n",
    "- [**Getting Started With Your First NIM**](#Getting-Started-With-Your-First-NIM)\n",
    "- [**End-To-End Latency versus Time-to-First-Token**](#End-To-End-Latency-versus-Time-to-First-Token)\n",
    "- [**[EXERCISE] Experimental Setup**](#[EXERCISE]-Experimental-Setup)\n",
    "- [**Next Steps**](#Next-Steps)\n",
    "\n",
    "## Notebook Source\n",
    "This notebook is part of the **NVIDIA Deep Learning Institute (DLI)** curriculum. You can find more information and additional resources at the [**NVIDIA DLI website**](https://www.nvidia.com/en-us/training/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a67aa",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Getting Started With Your First NIM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33fc6a",
   "metadata": {},
   "source": [
    "[**NVIDIA NIMs**](https://www.nvidia.com/en-us/ai/) are microservices that kickstart GPU-optimized processes and conform to your system requirements for optimized delivery. They can be tested at [**build.nvidia.com**](https://www.build.nvidia.com) and span single-LLM (Llama, Mixtral, Nemotron, ...), non-linguistic models (diffusion, vision, health, ...), and orchestration (retriever) configurations. Additionally, they can be accessed via [**NGC**](https://catalog.ngc.nvidia.com) and kickstarted with relative ease. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a308b2",
   "metadata": {},
   "source": [
    "- **To get started with this course:** A NIM has already been kickstarted for you and will be up soon! Feel free to check out [**99-Reading-Logs.ipynb**](./99-Reading-Logs.ipynb) to check on its status. To see more about how this particular instance was deployed, check out [**`composer/docker-compose.yml`**](composer/docker-compose.yml).\n",
    "- **To learn about how to set one up with regular docker access:** Please visit [**99-Deploy-Llama-NIM.ipynb**](./99-Deploy-Llama-NIM.ipynb) to see how you could take advantage of NIMs using base Docker. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2effa6",
   "metadata": {},
   "source": [
    "Once the service is live, we can check that NIM is available as a `nim` service running in the background from another container. From the host machine, you should change the URL from `nim` to `localhost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12a9177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"health.response\",\"message\":\"Service is ready.\"}"
     ]
    }
   ],
   "source": [
    "!curl http://nim:8000/v1/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a0d67",
   "metadata": {},
   "source": [
    "Assuming that the microservice is ready, we should get a response similar to `{\"object\": \"health-response\", \"message\": \"Service is ready.\"}`. If you do not see this, check back on your deployment in [**99-Deploy-Llama-NIM.ipynb**](./99-Deploy-Llama-NIM.ipynb) and see if something went wrong.\n",
    "\n",
    "Next, let's verify the model loaded into NIM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d03a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta/llama3-8b-instruct\n"
     ]
    }
   ],
   "source": [
    "!curl -s -X GET 'http://nim:8000/v1/models' | jq -r '.data[0].id'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc107a7",
   "metadata": {},
   "source": [
    "You should see that `meta/llama3-8b-instruct` is available, and we will call it to generate responses. Let's test it by performing a simple inference request via its most basic interface; a direct curl request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1845f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A Graphics Processing Unit (GPU) is a specialized electronic circuit designed to quickly manipulate and alter memory to accelerate the creation of images in a frame buffer intended\n",
      "CPU times: user 5.56 ms, sys: 290 μs, total: 5.85 ms\n",
      "Wall time: 443 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "# OPTION 1: Send requests in terminal via `curl`\n",
    "curl -s -X 'POST' \\\n",
    "    'http://nim:8000/v1/completions' \\\n",
    "    -H 'accept: application/json' \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -d '{\n",
    "\"model\": \"meta/llama3-8b-instruct\",\n",
    "\"prompt\": \"Could you explain what a GPU is?\",\n",
    "\"max_tokens\": 30\n",
    "}' | jq -r '.choices[0].text'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a029c2",
   "metadata": {},
   "source": [
    "<br>\n",
    "Wall time refers to the actual, real-world elapsed time from when a request is sent to the LLM until a response is received"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a821235a-8f59-441f-9a5a-71f82a1b172c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Feel free to modify the prompt of the number of max_tokens to produce different text. When increasing the number of max_tokens, you will notice that the response takes longer. For example, let's increase the number of max_tokens to 300 and time the request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91537295-bc5a-449c-a314-2c1ce10b5939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "A graphics processing unit (GPU) is a specialized electronic circuit designed to quickly manipulate and alter memory to accelerate the creation of images in a frame buffer intended\n",
      "CPU times: user 2.61 ms, sys: 227 μs, total: 2.83 ms\n",
      "Wall time: 384 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## OPTION 1: continue with `curl`\n",
    "# %%bash\n",
    "# TIMEFORMAT=\"%Es\" curl -s -X 'POST' \\ -s -X 'POST' \\\n",
    "#     'http://nim:8000/v1/completions' \\\n",
    "#     -H 'accept: application/json' \\\n",
    "#     -H 'Content-Type: application/json' \\\n",
    "#     -d '{\n",
    "# \"model\": \"meta/llama3-8b-instruct\",\n",
    "# \"prompt\": \"Could you explain what a GPU is?\",\n",
    "# \"max_tokens\": 300\n",
    "# }' | jq -r '.choices[0].text'\n",
    "\n",
    "## Option 2: Continue in python via `requests`\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://nim:8000/v1/completions\"\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"meta/llama3-8b-instruct\",\n",
    "    \"prompt\": \"Could you explain what a GPU is?\",\n",
    "    \"max_tokens\": 30,\n",
    "}\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "# for max token = 30\n",
    "if response.status_code == 200:\n",
    "    response_text = response.json().get('choices', [{}])[0].get('text', '').strip()\n",
    "    print(f\"Response:\\n{response_text}\")\n",
    "else:\n",
    "    print(f\"Failed to get a response, status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8883eefb-3c16-4f50-bab2-8b2252c72ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response2:\n",
      "A GPU, or Graphics Processing Unit, is a specialized electronic circuit designed to quickly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. In other words, it's a computer chip that's specifically designed to handle graphics and computationally intensive tasks.\n",
      "\n",
      "In the early days of computing, the CPU (Central Processing Unit) handled all the computations, including graphics rendering. However, as graphics became more complex and computationally intensive, it became clear that a separate chip was needed to handle these tasks. This is how the GPU was born.\n",
      "\n",
      "Today, GPUs are not just limited to graphics processing. They're also used for various applications such as:\n",
      "\n",
      "1. Scientific simulations\n",
      "2. Machine learning and artificial intelligence\n",
      "3. Cryptocurrencies mining\n",
      "4. Video editing and rendering\n",
      "5. 3D modeling and animation\n",
      "\n",
      "In summary, a GPU is a powerful electronic circuit designed to accelerate graphics and computationally intensive tasks, allowing for faster rendering, smoother visuals, and improved overall system performance.\n",
      "\n",
      "Would you like to know more about how GPUs work or some general information about GPU architectures? 🤔\n",
      "\n",
      "Type 'More' if you're interested in learning more! 👇\n",
      "\n",
      "Type 'None' if you're good with your understanding of GPUs for now. 👋\n",
      "\n",
      "(Note: You can also ask follow-up questions or share your thoughts about GPUs!) 💬\n",
      "```\n",
      "Please respond with 'More' or 'None'.  If you have\n",
      "CPU times: user 2.86 ms, sys: 242 μs, total: 3.1 ms\n",
      "Wall time: 3.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Option 2: Continue in python via `requests`\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://nim:8000/v1/completions\"\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data2 = {\n",
    "    \"model\": \"meta/llama3-8b-instruct\",\n",
    "    \"prompt\": \"Could you explain what a GPU is?\",\n",
    "    \"max_tokens\": 300,\n",
    "}\n",
    "response2 = requests.post(url, headers=headers, data=json.dumps(data2))\n",
    "\n",
    "# for max token = 300\n",
    "if response2.status_code == 200:\n",
    "    response_text2 = response2.json().get('choices', [{}])[0].get('text', '').strip()\n",
    "    print(f\"Response2:\\n{response_text2}\")\n",
    "else:\n",
    "    print(f\"Failed to get a response, status code: {response2.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea16f14c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In our setup, the longer response takes around 3.2s, vs 0.6s for the shorter one. Note that the `max_tokens` number of tokens may not be reached if an end-of-sequence token is returned by the LLM before. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b2f2da7-50ed-4730-8446-7a658ef33e3c",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **End-To-End Latency versus Time-to-First-Token**\n",
    "\n",
    "An inference request has two main stages: **prefill** and **decoding**.\n",
    "- During **prefill**, the LLM processes the prompt we send to the model, which in our example was \"Could you explain what a GPU is?\", and produces the first generated token.\n",
    "    - `TTFT`: **time-to-first-token**, or prefill duration.\n",
    "- During **decoding**, the LLM predicts the subsequent tokens one at a time, until reaching max_tokens or producing an end-of-sequence token. Prefill and decoding phases happen internally even if only the completed response is returned, as above.\n",
    "    - `E2E Latency`: **End-to-end latency** from the combined prefill and decoding stages.\n",
    "\n",
    "The latency you measured before was the `E2E Latency`, but it will be better for us to focus on `TTFT` for streaming applications like chatbots. Since the user can start reading the response as the LLM generates tokens, the user experience is acceptable as long as the speed of token generation is faster than the human reading speed. \n",
    "# Fun fact: Fast human reading speed is 90 ms/token (=500 words/minute at 0.75 tokens/word) (avg is 200 ms/token)\n",
    "\n",
    "Below, we connect to our NIM microservice via the OpenAI client for simplicity, swapping the `base_url` to our local endpoint and setting `stream=True` to invoke streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea2ef592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The origin story of the Graphics Processing Unit (GPU) dates back to the early days of computing. In the 1960s and 1970s, computers relied solely on Central Processing Units (CPUs) to handle all computing tasks, including graphics. However, as computer graphics began to take shape, the need for a specialized processing unit became apparent.\n",
      "\n",
      "In the late 1960s, the Stanford Research Institute (SRI) developed the first dedicated graphics processing unit, the SRI Graphics Processing Unit (GPU). This pioneering device was designed specifically for generating graphics, freeing up the CPU to focus on other tasks. The SRI GPU was a bulky device, consisting of a combination of discrete logic gates and memory, but it marked the beginning of a new era in computer graphics.\n",
      "\n",
      "Fast-forward to the 1980s, when the first NVIDIA GPU was developed. Founded in 1993, NVIDIA was initially a spinoff from a company called Inco Systems International, which focused on developing graphics processing units for the Atari 1024 graphics chip. The first NVIDIA GPU, the NVIDIA NV1, was released in 1995 and quickly gained popularity due to its advanced 3D graphics capabilities.\n",
      "\n",
      "As the 1990s progressed, NVIDIA continued to innovate and release new GPUs, each pushing the boundaries of 3D graphics. The NVIDIA RIVA 128, released in 1997, was a significant improvement over its predecessor, offering enhanced texture mapping and 3D acceleration.CPU times: user 327 ms, sys: 58.6 ms, total: 386 ms\n",
      "Wall time: 3.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import sys\n",
    "\n",
    "client = OpenAI(base_url=\"http://nim:8000/v1\", api_key=\"not-used\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta/llama3-8b-instruct\",\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Tell me a long story about GPUs'}\n",
    "    ],\n",
    "    max_tokens=300,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content is not None:\n",
    "        sys.stdout.write(content)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7583e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "*The Chunk output in response is*  **[ChatCompletionChunk(id='cmpl-978d4cd8d6804aadaef7f77fd1685fda', choices=[Choice(delta=ChoiceDelta(content='It', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1731387790, model='meta/llama3-8b-instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2961f-4fb1-4a21-8eb5-71c2f3e073fe",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "You can see that a user could enjoy a decent reading experience even as the model generates more and more of the response. This resembles the experience of interacting with an online LLM inference application like a chatbot, which explains why `TTFT` is so important there.\n",
    "\n",
    "Notice that our prompt was a very short \"Tell me a long story about GPUs\" request. As the length prompt increases, we should expect the `TTFT` to go up since more and more context tokens have to be propagated through our network. Perhaps the prompt length should be a factor in our timing considerations..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48d64f-dc61-4086-9584-402a5a668663",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## **[EXERCISE] Experimental Setup**\n",
    "\n",
    "To reinforce this point, let's modify the previous code to streamline experimentation. Please define the `measure_latency` command below to help you perform some simple timing experiments based on a set of reasonable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "823ed1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_latency(n_input_tokens: int = 50, max_output_tokens: int = 1, verbose = True):\n",
    "    # Let's define a dummy prompt with a variable number of input tokens\n",
    "    dummy_prompt = \" \".join([\"hi\"] * (n_input_tokens - 1))\n",
    "\n",
    "    ## TODO: Create a connector that connects to the nim endpoint\n",
    "    # client = None\n",
    "    client = OpenAI(base_url=\"http://nim:8000/v1\", api_key=\"not-used\")\n",
    "    \n",
    "    # Record the start time and (later) the end time of the simulation to compute duration\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ## TODO: Using client, send the request to the NIM\n",
    "    ## - Make sure to set max_tokens to stop generation\n",
    "    ## - Make sure stream=True as the following code expects a generator\n",
    "    ## - Set messages=[...] with the openai library\n",
    "    ##    - But if you notice you're not getting enough outputs, use \"prompt\": \"...\"  with the requests library\n",
    "    # response = None\n",
    "    response = client.chat.completions.create(\n",
    "                model=\"meta/llama3-8b-instruct\",\n",
    "                messages = [{'role': 'user', 'content': dummy_prompt}],\n",
    "                max_tokens=max_output_tokens,\n",
    "                stream=True\n",
    "            )\n",
    "    \n",
    "    ## Wait for the responses to come in, accumulating them along the way\n",
    "    ## NOTE: If using Chat endpoint, the first token is generally a confirmation empty-token. Best solution is to check if token has content\n",
    "    n_generated_tokens = 0\n",
    "    # n_generated_tokens = -1\n",
    "    for chunk in response:\n",
    "        n_generated_tokens += 1\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content is not None:\n",
    "            sys.stdout.write(content)\n",
    "            sys.stdout.flush()\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    if verbose: \n",
    "        print(f\"\\n{n_generated_tokens}-token latency with {n_input_tokens} input tokens is {duration:.2f}s\")\n",
    "    \n",
    "    return duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c497ede",
   "metadata": {},
   "source": [
    "The previous code measures the time to generate a response according to the specified `n_input_tokens` and `max_output_tokens`. We are not printing the response since we employ a dummy input prompt formed by repeated \"hi\"s. Note that the latency of the LLM doesn't depend on the type of token passed into it: as a result, it doesn't really matter what tokens are contained in the prompt. For latency purposes, the only important variable is the number of tokens in the prompt.\n",
    "Let's time the function call with the default `max_output_tokens=1` to measure the `TTFT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "efb7d822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n",
      "2-token latency with 50 input tokens is 0.02s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02406454086303711"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: Measure the latency of 50-in, 1-out\n",
    "measure_latency(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f234d",
   "metadata": {},
   "source": [
    "As the number of input tokens increases, so does the `TTFT`. For example, let's set 8000 input tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7874d21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "2-token latency with 8000 input tokens is 0.62s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6174023151397705"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: Measure the latency of 8000-in, 1-out\n",
    "measure_latency(8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695c39c",
   "metadata": {},
   "source": [
    "In our setup, the `TTFT` increased from 0.03s to 0.6s, which isn't too terrible but still a sizable increase. Now, consider what happens when we try to generate 500 tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1e87212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WOW! That's a lot of \"hi\"s!\n",
      "\n",
      "I'm happy to see so much enthusiasm and energy! Are you just having a fun day, or is there something specific you'd like to talk about or ask? I'm here to listen and help in any way I can!\n",
      "60-token latency with 50 input tokens is 0.76s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7626984119415283"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: Measure the latency of 50-in, 500-out\n",
    "measure_latency(50,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe657b8",
   "metadata": {},
   "source": [
    "**In our setup, assuming no extraneous load from other services/users:**\n",
    " - The `TTFT` for just 50 input tokens takes ~0.02s. There's very little prefill to process and very little decoding to do, so this makes a lot of sense. \n",
    " - The `TTFT` for 8000 input tokens takes ~0.6s. This is quite a reasonable number for user experience, but the impact is sizable.\n",
    " - The `E2E Latency` of just 50 input tokens but 500 output tokens takes ~6.6s. This is because the prefill stage is more efficient than decoding. This will be discussed in detail in the next notebook but has to do with the one-at-a-time nature of autoregressive decoding.\n",
    "\n",
    "That's why streaming is so powerful in improving user experience. We encourage you to try measuring the `TTFT` and `E2E Latency` for other use cases by changing `n_input_tokens` and `max_output_tokens` in the function `measure_latency()`.\n",
    "\n",
    "<details>\n",
    "<summary><b>Reveal Solution</b></summary>\n",
    "\n",
    "```python \n",
    "import time\n",
    "\n",
    "def measure_latency(n_input_tokens: int = 50, max_output_tokens: int = 1, verbose = True):\n",
    "    # Let's define a dummy prompt with a variable number of input tokens\n",
    "    dummy_prompt = \" \".join([\"hi\"] * (n_input_tokens - 1))\n",
    "\n",
    "    ## TODO: Create a connector that connects to the nim endpoint\n",
    "    client = OpenAI(base_url=\"http://nim:8000/v1\", api_key=\"not-used\")\n",
    "    \n",
    "    # Record the start time and (later) the end time of the simulation to compute duration\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ## TODO: Using client, send the request to the NIM\n",
    "    ## - Make sure to set max_tokens to stop generation\n",
    "    ## - Make sure stream=True as the following code expects a generator\n",
    "    ## - Set messages=[...] with the openai library\n",
    "    ##    - But if you encounter errors, use \"prompt\": \"...\"  with the requests library\n",
    "    # response = client.chat.completions.create(\n",
    "    response = client.completions.create(\n",
    "        model=\"meta/llama3-8b-instruct\",\n",
    "        # messages = [{'role': 'user', 'content': dummy_prompt}],\n",
    "        prompt = dummy_prompt,\n",
    "        max_tokens = max_output_tokens,\n",
    "        stream = True,\n",
    "    )\n",
    "    ## NOTE: With messages, models have strong priors to stop generating text \"in a conversational fashion\"\n",
    "    ## As such, trying to use messages here will result in shorter generations which will mess up the benchmarks.\n",
    "    ## Later notebooks address this issue by explicitly disabling end-of-string tokens under the hood. \n",
    "\n",
    "    ## Wait for the responses to come in, accumulating them along the way\n",
    "    ## NOTE: If using Chat endpoint, the first token is generally a confirmation empty-token. Best solution is to check if token has content\n",
    "    n_generated_tokens = 0\n",
    "    # n_generated_tokens = -1\n",
    "    for chunk in response:\n",
    "        n_generated_tokens += 1\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    if verbose: \n",
    "        print(f\"{n_generated_tokens}-token latency with {n_input_tokens} input tokens is {duration:.2f}s\")\n",
    "    \n",
    "    return duration\n",
    "\n",
    "## TODO: Measure the latency of 50-in, 1-out\n",
    "measure_latency(50);\n",
    "\n",
    "## TODO: Measure the latency of 8000-in, 1-out\n",
    "measure_latency(8000);\n",
    "\n",
    "## TODO: Measure the latency of 50-in, 500-out\n",
    "measure_latency(50, 500);\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e826c0a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## **Next Steps**\n",
    "\n",
    "Great job completing this notebook! You've successfully learned how to interact with a NIM endpoint, check its status, query available models, and generate text using both curl and Python. Understanding the differences between end-to-end latency and time-to-first-token will help you optimize your applications for better performance. In the next notebook, we will be looking into the trade-offs in much more depth and will consider how they factor into large-scale system designs. \n",
    "\n",
    "- **When you're ready, feel free to go on to the next video and the notebook that follows!**\n",
    "- The overall workflow for the course is as follows: **Watch the video -> Explore the notebook -> Repeat**\n",
    "- **Feel free to keep the environment active during this time, but please download your work and stop the environment if you decide to take a break.** This will help you to retain your progress and compute resources for later spin-ups.\n",
    "\n",
    "**With that said, feel free to move on to the next video, and enjoy the course!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa6cf0-7f17-4d8f-8ebb-9785a1ac8a4a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f1fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/trt_llm_0_9_0_dli.csv')\n",
    "df.to_excel('dataset/trt.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0056afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/nim_dli.csv')\n",
    "df.to_excel('dataset/nim.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19a215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
